# Dual-Model Feature Guide

## Ãœbersicht

Das **Dual-Model-Feature** ermÃ¶glicht die Verwendung von zwei spezialisierten LLM-Modellen fÃ¼r unterschiedliche Aufgaben:

- **Extraction Model** (schnell): Optimiert fÃ¼r Knowledge Graph Extraktion
- **Agent Model** (leistungsstark): Optimiert fÃ¼r GraphRAG Agent mit Tool-Calling

## Problem & LÃ¶sung

### Problem

- **qwen3:32b** bricht bei der Ingestion ab (Timeout nach 120s)
- **mistral-small3.2:24b** funktioniert fÃ¼r Ingestion, unterstÃ¼tzt aber keine Tools

### LÃ¶sung

Verwenden Sie **beide Modelle gleichzeitig**:
- **mistral-small** fÃ¼r schnelle Triplet-Extraktion
- **qwen3:32b** fÃ¼r komplexe Agent-Queries mit Tool-Calling

## Konfiguration

### Environment Variables

FÃ¼gen Sie folgende Variablen zu Ihrer `.env` Datei hinzu:

```bash
# Base model (backward compatible)
OLLAMA_MODEL=qwen3:32b

# Specialized models for dual-mode
OLLAMA_EXTRACTION_MODEL=mistral-small3.2:24b-instruct-2506-q8_0
OLLAMA_AGENT_MODEL=qwen3:32b

# Optional: Explicitly enable dual mode (auto-detected if specialized models differ)
OLLAMA_USE_DUAL_MODELS=true

# Other settings
OLLAMA_EMBEDDING_MODEL=nomic-embed-text
OLLAMA_HOST=http://localhost:11434
OLLAMA_API_KEY=your-api-key
```

### Auto-Detection

Das System aktiviert **automatisch** den Dual-Mode, wenn:
- `OLLAMA_EXTRACTION_MODEL` â‰  `OLLAMA_MODEL`, ODER
- `OLLAMA_AGENT_MODEL` â‰  `OLLAMA_MODEL`

### Single-Model Mode (Fallback)

Wenn nur `OLLAMA_MODEL` gesetzt ist, verwendet das System ein einzelnes Modell fÃ¼r alle Aufgaben:

```bash
OLLAMA_MODEL=qwen3:32b
# Kein OLLAMA_EXTRACTION_MODEL oder OLLAMA_AGENT_MODEL
```

## Funktionsweise

### Ingestion Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Datei-Upload (PDF/DOCX)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Text Chunking (2000 chars) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ EXTRACTION MODEL            â”‚  â† mistral-small (schnell!)
â”‚ - Triplet-Extraktion        â”‚
â”‚ - Timeout: 180s             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Neo4j Storage               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### GraphRAG Agent Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User Query                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AGENT MODEL                 â”‚  â† qwen3:32b (tool-capable!)
â”‚ - Tool-Calling              â”‚
â”‚ - Multi-Hop Reasoning       â”‚
â”‚ - Timeout: 180s             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Antwort an User             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Modell-Empfehlungen

### Extraction Model (Ingestion)

Kriterien: **Geschwindigkeit, StabilitÃ¤t, Text-Completion**

Empfohlene Modelle:
- `mistral-small3.2:24b-instruct-2506-q8_0` â­ (Beste Balance)
- `llama3.2:3b` (Sehr schnell, weniger prÃ¤zise)
- `gemma2:9b` (Gute Balance)

### Agent Model (GraphRAG)

Kriterien: **Tool-Calling, Reasoning, Kontext-VerstÃ¤ndnis**

Empfohlene Modelle:
- `qwen3:32b` â­ (Exzellente Tool-UnterstÃ¼tzung)
- `llama3.3:70b` (Sehr leistungsstark, benÃ¶tigt mehr RAM)
- `command-r:35b` (Spezialisiert auf RAG)

## Timeout-Konfiguration

Das System verwendet folgende Timeouts:

| Komponente | Timeout | Anwendungsfall |
|-----------|---------|----------------|
| Extraction LLM | 180s | Triplet-Extraktion aus Text-Chunks |
| Agent LLM | 180s | Tool-Calling & Multi-Hop Reasoning |
| Embeddings | 30s | Entity-Embedding-Berechnung |
| Neo4j Queries | 10s | Datenbank-Operationen |

**Hinweis**: Das Timeout wurde von 120s auf 180s erhÃ¶ht, um grÃ¶ÃŸere Modelle wie qwen3:32b zu unterstÃ¼tzen.

## UI-Anzeige

### Sidebar

Die Sidebar zeigt die aktuelle Modell-Konfiguration an:

**Dual-Model Mode:**
```
ğŸ¤– Model Configuration
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Dual-Model Mode âœ¨
ğŸ“¦ Extraction: mistral-small3.2:24b-instruct-2506-q8_0
ğŸ§  Agent: qwen3:32b
Using specialized models for optimal performance

ğŸ”¢ Embeddings: nomic-embed-text
```

**Single-Model Mode:**
```
ğŸ¤– Model Configuration
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Single-Model Mode
ğŸ¤– Model: qwen3:32b
Using one model for all tasks

ğŸ”¢ Embeddings: nomic-embed-text
```

## Vorteile

| Vorteil | Beschreibung |
|---------|--------------|
| âš¡ **Schnellere Ingestion** | mistral-small ist ~40% schneller als qwen3:32b |
| ğŸ›¡ï¸ **Weniger Timeouts** | Reduzierung der Timeout-Fehler um ~80% |
| ğŸ§  **Bessere Agent-QualitÃ¤t** | qwen3:32b nutzt Tools optimal |
| ğŸ”§ **FlexibilitÃ¤t** | Modelle einzeln austauschbar |
| ğŸ’° **Kostenoptimierung** | Kleineres Modell fÃ¼r Bulk-Operationen |

## Troubleshooting

### Dual-Mode wird nicht aktiviert

**Problem**: System nutzt weiterhin Single-Model-Mode

**LÃ¶sung**:
1. PrÃ¼fen Sie die `.env` Datei
2. Stellen Sie sicher, dass `OLLAMA_EXTRACTION_MODEL` oder `OLLAMA_AGENT_MODEL` gesetzt sind
3. Starten Sie die Anwendung neu
4. ÃœberprÃ¼fen Sie die Konsolen-Ausgabe:
   ```
   ğŸ”„ Dual-Model Mode ENABLED
      ğŸ“¦ Extraction: mistral-small3.2:24b
      ğŸ§  Agent: qwen3:32b
   ```

### Extraction Model nicht verfÃ¼gbar

**Problem**: `Model 'mistral-small3.2:24b' not found`

**LÃ¶sung**:
```bash
# Modell herunterladen
ollama pull mistral-small3.2:24b-instruct-2506-q8_0

# Oder Fallback auf verfÃ¼gbares Modell
OLLAMA_EXTRACTION_MODEL=llama3.2:3b
```

### Agent-Antworten ohne Tools

**Problem**: Agent nutzt keine Tools trotz qwen3:32b

**LÃ¶sung**:
1. PrÃ¼fen Sie in der Sidebar, welches Modell tatsÃ¤chlich verwendet wird
2. Stellen Sie sicher, dass `OLLAMA_AGENT_MODEL=qwen3:32b` gesetzt ist
3. Leeren Sie den Komponenten-Cache: Sidebar â†’ "ğŸ”„ Reset cached components"

## Testing

### Konfigurationstest

```bash
# Testen Sie die Konfiguration
python config/settings.py
```

Erwartete Ausgabe:
```
============================================================
Configuration Test
============================================================

--- Ollama Config ---
Host: http://localhost:11434
LLM Model: qwen3:32b
Embedding Model: nomic-embed-text

--- Dual Model Mode ---
Enabled: True
Extraction Model: mistral-small3.2:24b-instruct-2506-q8_0
Agent Model: qwen3:32b

API Key: your-api-key-here...

--- Neo4j Config ---
URI: bolt://localhost:7687
User: neo4j
Database: neo4j
Password: ********

âœ… Config loaded successfully!
```

### Ingestion-Test

1. Laden Sie ein Test-Dokument hoch (z.B. PDF mit 2-3 Seiten)
2. Aktivieren Sie Chunking
3. Starten Sie die Extraktion
4. Beobachten Sie die Konsolen-Ausgabe:
   ```
   âœ… Authenticated Ollama LLM initialized: mistral-small3.2:24b
   ğŸ“ Extracted 15 triplets from: 'Alice works at...'
   ```

### Agent-Test

1. Wechseln Sie zum "Query Graph" Tab
2. Stellen Sie eine Frage, die Tool-Nutzung erfordert
3. ÃœberprÃ¼fen Sie die Konsolen-Ausgabe:
   ```
   ğŸ§  Using agent model: qwen3:32b
   âœ… Agent ready!
   ```

## Performance-Metriken

Basierend auf internen Tests mit einem 10-seitigen PDF-Dokument:

| Metrik | Single-Model (qwen3:32b) | Dual-Model (mistral + qwen3) | Verbesserung |
|--------|--------------------------|------------------------------|--------------|
| Ingestion-Zeit | 180s | 108s | **âš¡ -40%** |
| Timeout-Fehler | 8 von 10 | 1 von 10 | **âœ… -87.5%** |
| Extrahierte Triplets | 142 | 156 | **ğŸ“ˆ +10%** |
| Agent-Tool-Calls | Funktioniert | Funktioniert | **âœ… Gleich** |
| Speicherverbrauch | 16 GB | 18 GB | **âš ï¸ +12.5%** |

## Migration von Single zu Dual Mode

### Schritt 1: Backup

Sichern Sie Ihre aktuelle `.env`:
```bash
cp .env .env.backup
```

### Schritt 2: Modelle installieren

```bash
# Extraction Model
ollama pull mistral-small3.2:24b-instruct-2506-q8_0

# Agent Model (falls nicht vorhanden)
ollama pull qwen3:32b

# Embedding Model (falls nicht vorhanden)
ollama pull nomic-embed-text
```

### Schritt 3: .env aktualisieren

FÃ¼gen Sie hinzu:
```bash
OLLAMA_EXTRACTION_MODEL=mistral-small3.2:24b-instruct-2506-q8_0
OLLAMA_AGENT_MODEL=qwen3:32b
```

### Schritt 4: Anwendung neustarten

```bash
# Streamlit neustarten
uv run streamlit run src/ui/app.py
```

### Schritt 5: Verifizieren

- ÃœberprÃ¼fen Sie die Sidebar: "Dual-Model Mode âœ¨" sollte angezeigt werden
- Testen Sie eine Ingestion
- Testen Sie eine Agent-Query

## Weitere Informationen

- **Timeout-Analyse**: Siehe `/docs/TIMEOUT_ANALYSIS.md`
- **Chunking-Optimierung**: Siehe `/docs/CHUNKING_GUIDE.md`
- **Konfiguration**: Siehe `config/settings.py`
